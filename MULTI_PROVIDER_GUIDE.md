# Multi-Provider LLM Guide

The Analyst Agent now supports **multiple LLM providers** with automatic fallback, making it flexible and resilient across different AI services.

## üéØ Supported Providers

### OpenAI
- **Models**: `gpt-4`, `gpt-4-turbo`, `gpt-4o`, `gpt-3.5-turbo`
- **Setup**: Set `OPENAI_API_KEY` environment variable
- **Best for**: General SQL generation, reliable performance

### Anthropic (Claude)
- **Models**: Claude 3 Opus, Sonnet, Haiku (auto-mapped from OpenAI model names)
- **Setup**: Set `ANTHROPIC_API_KEY` environment variable  
- **Best for**: Complex reasoning, safety-focused applications

### Local Models
- **Models**: Any Ollama-supported model (Llama, Mistral, etc.)
- **Setup**: Install Ollama locally
- **Best for**: Privacy, cost control, offline usage

## üöÄ Quick Setup

### 1. Environment Variables

Create or update your `.env` file:

```bash
# Primary provider
DEFAULT_LLM_PROVIDER=openai
DEFAULT_LLM_MODEL=gpt-4

# OpenAI (recommended)
OPENAI_API_KEY=sk-your-openai-key-here

# Anthropic (optional, for fallback)
ANTHROPIC_API_KEY=sk-ant-your-anthropic-key-here

# Other settings
DEBUG=false
API_PORT=8000
```

### 2. Install Provider Dependencies

```bash
# OpenAI (included by default)
pip install langchain-openai

# Anthropic (optional)
pip install langchain-anthropic

# Local models via Ollama (optional)
pip install langchain-community
# Then install Ollama: https://ollama.ai
```

## üîß Configuration Options

### Provider Priority

The system automatically uses providers in this order:
1. **Configured provider** (`DEFAULT_LLM_PROVIDER`)
2. **Available fallbacks** (based on API keys)
3. **Local models** (if Ollama is installed)

### Model Mapping

When using Anthropic, OpenAI model names are automatically mapped:

```python
# OpenAI ‚Üí Anthropic mapping
"gpt-4" ‚Üí "claude-3-opus-20240229"
"gpt-4-turbo" ‚Üí "claude-3-sonnet-20240229"
"gpt-3.5-turbo" ‚Üí "claude-3-haiku-20240307"
"gpt-4o" ‚Üí "claude-3-5-sonnet-20241022"
```

## üíª Usage Examples

### Automatic Provider Selection

```python
from analyst_agent.core.llm_factory import LLMFactory

# Uses default provider from settings
llm = LLMFactory.create_llm()
```

### Explicit Provider Selection

```python
# Force OpenAI
openai_llm = LLMFactory.create_llm(provider="openai", model="gpt-4")

# Force Anthropic  
anthropic_llm = LLMFactory.create_llm(provider="anthropic", model="gpt-4")

# Local model
local_llm = LLMFactory.create_llm(provider="local", model="llama2")
```

### Check Available Providers

```python
providers = LLMFactory.get_available_providers()
print(f"Available: {providers}")
# Output: ['openai', 'anthropic'] (based on API keys)
```

## üîÑ Automatic Fallback

The system includes **intelligent fallback logic**:

1. **Primary fails** ‚Üí Try next available provider
2. **All providers fail** ‚Üí Raise clear error message
3. **No API keys** ‚Üí Skip unavailable providers
4. **Network issues** ‚Üí Automatic retry with different provider

Example fallback flow:
```
OpenAI (primary) ‚Üí Anthropic (fallback) ‚Üí Local (last resort)
```

## üõ°Ô∏è Error Handling

### Graceful Degradation

```python
try:
    llm = LLMFactory.create_llm()
    result = llm.invoke("Generate SQL for user count")
except Exception as e:
    # Fallback to different provider automatically handled
    # Only fails if NO providers are available
    print(f"All LLM providers failed: {e}")
```

### Provider-Specific Errors

The system logs detailed error information:

```bash
2025-07-27 [warning] Primary provider failed, trying fallbacks provider=openai
2025-07-27 [info] Using fallback provider original=openai fallback=anthropic model=gpt-4
```

## üß™ Testing

Run the multi-provider test:

```bash
python examples/multi_provider_test.py
```

This will:
- ‚úÖ Detect available providers
- ‚úÖ Test LLM instance creation
- ‚úÖ Test SQL generation
- ‚úÖ Test fallback logic

## üéõÔ∏è Advanced Configuration

### Custom Provider Settings

```python
# Custom temperature and parameters
llm = LLMFactory.create_llm(
    provider="anthropic",
    model="gpt-4",
    temperature=0.1,
    max_tokens=4000
)
```

### Provider Caching

LLM instances are cached for performance:

```python
# Clear cache if needed
LLMFactory.clear_cache()
```

### Docker Configuration

Update your `docker-compose.yml`:

```yaml
services:
  analyst-agent:
    environment:
      # Multi-provider setup
      - OPENAI_API_KEY=${OPENAI_API_KEY}
      - ANTHROPIC_API_KEY=${ANTHROPIC_API_KEY}
      - DEFAULT_LLM_PROVIDER=openai
      - DEFAULT_LLM_MODEL=gpt-4
```

## üö® Troubleshooting

### No Providers Available

```bash
‚ùå No available LLM providers. Check your API keys.
```

**Solution**: Add at least one API key to your `.env` file.

### Import Errors

```bash
‚ùå LLM provider library not installed: anthropic
```

**Solution**: Install the required package:
```bash
pip install langchain-anthropic
```

### Model Not Found

```bash
‚ùå Model 'gpt-5' not found
```

**Solution**: Use supported model names or check provider documentation.

## üìä Cost Optimization

### Provider Selection by Cost

| Provider | Cost (approx.) | Speed | Quality |
|----------|----------------|-------|---------|
| OpenAI GPT-3.5 | $ | Fast | Good |
| OpenAI GPT-4 | $$$ | Medium | Excellent |
| Anthropic Claude | $$ | Medium | Excellent |
| Local (Ollama) | Free | Varies | Good |

### Recommendations

- **Production**: OpenAI GPT-4 (primary) + Anthropic (fallback)
- **Development**: OpenAI GPT-3.5-turbo (cost-effective)
- **Privacy-focused**: Local models only
- **High-availability**: All providers configured

## üîÆ Future Providers

The architecture supports easy addition of new providers:

- Google Gemini
- Azure OpenAI
- AWS Bedrock
- Cohere
- Custom API endpoints

---

## üéØ Summary

‚úÖ **Multiple providers supported** (OpenAI, Anthropic, Local)  
‚úÖ **Automatic fallback** when primary provider fails  
‚úÖ **Intelligent model mapping** between providers  
‚úÖ **Environment-based configuration**  
‚úÖ **Comprehensive error handling**  
‚úÖ **Performance caching**  
‚úÖ **Easy testing and validation**  

The system is now **provider-agnostic** and can adapt to any LLM service, making your analyst agent **resilient and flexible**! üöÄ 